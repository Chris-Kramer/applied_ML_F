{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Catboost use different names for parameters compared to sklearns model.  \n",
    "Here is a nice guide for catboost https://www.kaggle.com/code/mitribunskiy/tutorial-catboost-overview/notebook  \n",
    "Here is likewise a nice guide: https://coderzcolumn.com/tutorials/machine-learning/catboost-an-in-depth-guide-python#2  \n",
    "Otherwise the official homepage have more information https://catboost.ai/en/docs/  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "# Data wrangling\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "# Model\n",
    "from catboost import CatBoostClassifier\n",
    "# performance measure\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Grid search\n",
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ Load data ------\n",
    "# test data (no labels (y_test) since we that way can't see the results)\n",
    "X_test = np.load(\"../Common/data/X_test.npy\")\n",
    "# Validation and training\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "                                                  np.load(\"../Common/data/X_train.npy\"), # X data\n",
    "                                                  np.load(\"../Common/data/y_train.npy\"), # y data (labels)\n",
    "                                                  test_size = 0.5,\n",
    "                                                  random_state = 42\n",
    "                                                  )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing around  \n",
    "This is just a bit of playing, next section I will do grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0b15a1bffa042b68804552da0bdeabe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "MetricVisualizer(layout=Layout(align_self='stretch', height='500px'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0:\ttest: 0.7793054\tbest: 0.7793054 (0)\ttotal: 37.1ms\tremaining: 9.23s\n",
      "100:\ttest: 0.9494087\tbest: 0.9494087 (100)\ttotal: 2.63s\tremaining: 3.88s\n",
      "Stopped by overfitting detector  (10 iterations wait)\n",
      "\n",
      "bestTest = 0.9496872283\n",
      "bestIteration = 111\n",
      "\n",
      "Shrink model to first 112 iterations.\n",
      "87.57083333333333\n"
     ]
    }
   ],
   "source": [
    "# ------ Create and test model ------\n",
    "model = CatBoostClassifier(\n",
    "                           n_estimators = 1000, # Default = 1000 - How many slow learners to have in the forrest (also called iterations)\n",
    "                           max_leaves = 31, # Default = 31 - The same as max_features in sklearn (how many features to consider)\n",
    "                           min_data_in_leaf = 1, # Default = 1 - The same as min_samples_leaf (The minimum number of samples to have in a leaf)\n",
    "                           max_depth  = 6, # Default = 6 - The max depth of our slow learners \n",
    "                           learning_rate= 0.3, # Default = 0.03 - The learning rate for gradient descent algorithm\n",
    "                           verbose = 100, # Print training process every 100 iteration\n",
    "                           random_state = 42, # For reproducibility\n",
    "                           early_stopping_rounds = 10, #, No default - Preventing overfitting\n",
    "                           bootstrap_type= \"No\",  # I think default it No bootstrapping Bayesian\n",
    "                           eval_metric= 'AUC' # Default: Logloss - Can either evaluate on the loss function or the auccuracy\n",
    "                           loss_function= \"MultiClass\" #  For 2-class classification use 'LogLoss' or 'CrossEntropy'. For multiclass use 'MultiClass'\n",
    "                           )\n",
    "\n",
    "model.fit(X_val, y_val, eval_set=(X_train, y_train), plot = True)\n",
    "y_train_hat = model.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, y_train_hat)\n",
    "print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid search  \n",
    "Here I will perform grid search for the final model.  \n",
    "Here you can read more: https://effectiveml.com/using-grid-search-to-optimise-catboost-parameters.html  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters to perform grid-search on:\n",
    "model_params_search = {\n",
    "            \"n_estimators\": [250, 500, 1000], # Default = 1000 - How many slow learners to have in the forrest\n",
    "            \"max_leaves\": [10, 31, 50], # Default = 31 - The same as max_features in sklearn (how many features to consider)\n",
    "            \"min_data_in_leaf\": [1, 2, 3], # Default = 1 - The same as min_samples_leaf (The minimum number of samples to have in a leaf)\n",
    "            \"max_depth\": [1, 2, 6], # Default = 6 - The max depth of our slow learners \n",
    "            \"learning_rate\":  [0.001, 0.01, 0.3] , # Default = 0.03 - The learning rate for gradient descent algorithm\n",
    "            \"bootstrap_type\": [\"Bayesian\", \"Bernoulli\", \"MVS\"],  # I think default it No bootstrapping\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [4], line 18\u001b[0m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;66;03m# Grid search\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# found the guide here: https://youtu.be/N4rqz8Z4XOM \u001b[39;00m\n\u001b[0;32m      9\u001b[0m grid_search \u001b[38;5;241m=\u001b[39m GridSearchCV(\n\u001b[0;32m     10\u001b[0m                            estimator\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     11\u001b[0m                            param_grid \u001b[38;5;241m=\u001b[39m model_params_search,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     15\u001b[0m                            error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     16\u001b[0m                            )\n\u001b[1;32m---> 18\u001b[0m grid_result \u001b[38;5;241m=\u001b[39m grid_search\u001b[38;5;241m.\u001b[39mfit(X_val, y_val)\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:875\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    869\u001b[0m     results \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_format_results(\n\u001b[0;32m    870\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    871\u001b[0m     )\n\u001b[0;32m    873\u001b[0m     \u001b[39mreturn\u001b[39;00m results\n\u001b[1;32m--> 875\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_run_search(evaluate_candidates)\n\u001b[0;32m    877\u001b[0m \u001b[39m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    878\u001b[0m \u001b[39m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    879\u001b[0m first_test_score \u001b[39m=\u001b[39m all_out[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mtest_scores\u001b[39m\u001b[39m\"\u001b[39m]\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1379\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1377\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_run_search\u001b[39m(\u001b[39mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1378\u001b[0m     \u001b[39m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1379\u001b[0m     evaluate_candidates(ParameterGrid(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mparam_grid))\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_search.py:822\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    814\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m    815\u001b[0m     \u001b[39mprint\u001b[39m(\n\u001b[0;32m    816\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mFitting \u001b[39m\u001b[39m{0}\u001b[39;00m\u001b[39m folds for each of \u001b[39m\u001b[39m{1}\u001b[39;00m\u001b[39m candidates,\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    817\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m totalling \u001b[39m\u001b[39m{2}\u001b[39;00m\u001b[39m fits\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.\u001b[39mformat(\n\u001b[0;32m    818\u001b[0m             n_splits, n_candidates, n_candidates \u001b[39m*\u001b[39m n_splits\n\u001b[0;32m    819\u001b[0m         )\n\u001b[0;32m    820\u001b[0m     )\n\u001b[1;32m--> 822\u001b[0m out \u001b[39m=\u001b[39m parallel(\n\u001b[0;32m    823\u001b[0m     delayed(_fit_and_score)(\n\u001b[0;32m    824\u001b[0m         clone(base_estimator),\n\u001b[0;32m    825\u001b[0m         X,\n\u001b[0;32m    826\u001b[0m         y,\n\u001b[0;32m    827\u001b[0m         train\u001b[39m=\u001b[39;49mtrain,\n\u001b[0;32m    828\u001b[0m         test\u001b[39m=\u001b[39;49mtest,\n\u001b[0;32m    829\u001b[0m         parameters\u001b[39m=\u001b[39;49mparameters,\n\u001b[0;32m    830\u001b[0m         split_progress\u001b[39m=\u001b[39;49m(split_idx, n_splits),\n\u001b[0;32m    831\u001b[0m         candidate_progress\u001b[39m=\u001b[39;49m(cand_idx, n_candidates),\n\u001b[0;32m    832\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_and_score_kwargs,\n\u001b[0;32m    833\u001b[0m     )\n\u001b[0;32m    834\u001b[0m     \u001b[39mfor\u001b[39;49;00m (cand_idx, parameters), (split_idx, (train, test)) \u001b[39min\u001b[39;49;00m product(\n\u001b[0;32m    835\u001b[0m         \u001b[39menumerate\u001b[39;49m(candidate_params), \u001b[39menumerate\u001b[39;49m(cv\u001b[39m.\u001b[39;49msplit(X, y, groups))\n\u001b[0;32m    836\u001b[0m     )\n\u001b[0;32m    837\u001b[0m )\n\u001b[0;32m    839\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(out) \u001b[39m<\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m    840\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    841\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mNo fits were performed. \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWas the CV iterator empty? \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mWere there no candidates?\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    844\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\joblib\\parallel.py:1088\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1085\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdispatch_one_batch(iterator):\n\u001b[0;32m   1086\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_iterating \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_original_iterator \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 1088\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdispatch_one_batch(iterator):\n\u001b[0;32m   1089\u001b[0m     \u001b[39mpass\u001b[39;00m\n\u001b[0;32m   1091\u001b[0m \u001b[39mif\u001b[39;00m pre_dispatch \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mall\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mor\u001b[39;00m n_jobs \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1092\u001b[0m     \u001b[39m# The iterable was consumed all at once by the above for loop.\u001b[39;00m\n\u001b[0;32m   1093\u001b[0m     \u001b[39m# No need to wait for async callbacks to trigger to\u001b[39;00m\n\u001b[0;32m   1094\u001b[0m     \u001b[39m# consumption.\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\joblib\\parallel.py:901\u001b[0m, in \u001b[0;36mParallel.dispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    899\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mFalse\u001b[39;00m\n\u001b[0;32m    900\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 901\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dispatch(tasks)\n\u001b[0;32m    902\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\joblib\\parallel.py:819\u001b[0m, in \u001b[0;36mParallel._dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    817\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    818\u001b[0m     job_idx \u001b[39m=\u001b[39m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs)\n\u001b[1;32m--> 819\u001b[0m     job \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_backend\u001b[39m.\u001b[39;49mapply_async(batch, callback\u001b[39m=\u001b[39;49mcb)\n\u001b[0;32m    820\u001b[0m     \u001b[39m# A job can complete so quickly than its callback is\u001b[39;00m\n\u001b[0;32m    821\u001b[0m     \u001b[39m# called before we get here, causing self._jobs to\u001b[39;00m\n\u001b[0;32m    822\u001b[0m     \u001b[39m# grow. To ensure correct results ordering, .insert is\u001b[39;00m\n\u001b[0;32m    823\u001b[0m     \u001b[39m# used (rather than .append) in the following line\u001b[39;00m\n\u001b[0;32m    824\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_jobs\u001b[39m.\u001b[39minsert(job_idx, job)\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\joblib\\_parallel_backends.py:208\u001b[0m, in \u001b[0;36mSequentialBackend.apply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply_async\u001b[39m(\u001b[39mself\u001b[39m, func, callback\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    207\u001b[0m     \u001b[39m\"\"\"Schedule a func to be run\"\"\"\u001b[39;00m\n\u001b[1;32m--> 208\u001b[0m     result \u001b[39m=\u001b[39m ImmediateResult(func)\n\u001b[0;32m    209\u001b[0m     \u001b[39mif\u001b[39;00m callback:\n\u001b[0;32m    210\u001b[0m         callback(result)\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\joblib\\_parallel_backends.py:597\u001b[0m, in \u001b[0;36mImmediateResult.__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, batch):\n\u001b[0;32m    595\u001b[0m     \u001b[39m# Don't delay the application, to avoid keeping the input\u001b[39;00m\n\u001b[0;32m    596\u001b[0m     \u001b[39m# arguments in memory\u001b[39;00m\n\u001b[1;32m--> 597\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mresults \u001b[39m=\u001b[39m batch()\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36mBatchedCalls.__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\joblib\\parallel.py:288\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m    285\u001b[0m     \u001b[39m# Set the default nested backend to self._backend but do not set the\u001b[39;00m\n\u001b[0;32m    286\u001b[0m     \u001b[39m# change the default number of processes to -1\u001b[39;00m\n\u001b[0;32m    287\u001b[0m     \u001b[39mwith\u001b[39;00m parallel_backend(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backend, n_jobs\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_n_jobs):\n\u001b[1;32m--> 288\u001b[0m         \u001b[39mreturn\u001b[39;00m [func(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m    289\u001b[0m                 \u001b[39mfor\u001b[39;00m func, args, kwargs \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mitems]\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\sklearn\\utils\\fixes.py:117\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    115\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    116\u001b[0m     \u001b[39mwith\u001b[39;00m config_context(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig):\n\u001b[1;32m--> 117\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfunction(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:708\u001b[0m, in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    705\u001b[0m result[\u001b[39m\"\u001b[39m\u001b[39mfit_error\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    707\u001b[0m fit_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time\n\u001b[1;32m--> 708\u001b[0m test_scores \u001b[39m=\u001b[39m _score(estimator, X_test, y_test, scorer, error_score)\n\u001b[0;32m    709\u001b[0m score_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start_time \u001b[39m-\u001b[39m fit_time\n\u001b[0;32m    710\u001b[0m \u001b[39mif\u001b[39;00m return_train_score:\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:767\u001b[0m, in \u001b[0;36m_score\u001b[1;34m(estimator, X_test, y_test, scorer, error_score)\u001b[0m\n\u001b[0;32m    765\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test)\n\u001b[0;32m    766\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 767\u001b[0m         scores \u001b[39m=\u001b[39m scorer(estimator, X_test, y_test)\n\u001b[0;32m    768\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[0;32m    769\u001b[0m     \u001b[39mif\u001b[39;00m error_score \u001b[39m==\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mraise\u001b[39m\u001b[39m\"\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:219\u001b[0m, in \u001b[0;36m_BaseScorer.__call__\u001b[1;34m(self, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    196\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__call__\u001b[39m(\u001b[39mself\u001b[39m, estimator, X, y_true, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    197\u001b[0m     \u001b[39m\"\"\"Evaluate predicted target values for X relative to y_true.\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \n\u001b[0;32m    199\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    217\u001b[0m \u001b[39m        Score function applied to prediction of estimator on X.\u001b[39;00m\n\u001b[0;32m    218\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 219\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_score(\n\u001b[0;32m    220\u001b[0m         partial(_cached_call, \u001b[39mNone\u001b[39;49;00m),\n\u001b[0;32m    221\u001b[0m         estimator,\n\u001b[0;32m    222\u001b[0m         X,\n\u001b[0;32m    223\u001b[0m         y_true,\n\u001b[0;32m    224\u001b[0m         sample_weight\u001b[39m=\u001b[39;49msample_weight,\n\u001b[0;32m    225\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:261\u001b[0m, in \u001b[0;36m_PredictScorer._score\u001b[1;34m(self, method_caller, estimator, X, y_true, sample_weight)\u001b[0m\n\u001b[0;32m    233\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_score\u001b[39m(\u001b[39mself\u001b[39m, method_caller, estimator, X, y_true, sample_weight\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m    234\u001b[0m     \u001b[39m\"\"\"Evaluate predicted target values for X relative to y_true.\u001b[39;00m\n\u001b[0;32m    235\u001b[0m \n\u001b[0;32m    236\u001b[0m \u001b[39m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    258\u001b[0m \u001b[39m        Score function applied to prediction of estimator on X.\u001b[39;00m\n\u001b[0;32m    259\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 261\u001b[0m     y_pred \u001b[39m=\u001b[39m method_caller(estimator, \u001b[39m\"\u001b[39;49m\u001b[39mpredict\u001b[39;49m\u001b[39m\"\u001b[39;49m, X)\n\u001b[0;32m    262\u001b[0m     \u001b[39mif\u001b[39;00m sample_weight \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    263\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sign \u001b[39m*\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_score_func(\n\u001b[0;32m    264\u001b[0m             y_true, y_pred, sample_weight\u001b[39m=\u001b[39msample_weight, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_kwargs\n\u001b[0;32m    265\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\sklearn\\metrics\\_scorer.py:71\u001b[0m, in \u001b[0;36m_cached_call\u001b[1;34m(cache, estimator, method, *args, **kwargs)\u001b[0m\n\u001b[0;32m     69\u001b[0m \u001b[39m\"\"\"Call estimator with method and args and kwargs.\"\"\"\u001b[39;00m\n\u001b[0;32m     70\u001b[0m \u001b[39mif\u001b[39;00m cache \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m---> 71\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mgetattr\u001b[39m(estimator, method)(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m     73\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m     74\u001b[0m     \u001b[39mreturn\u001b[39;00m cache[method]\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\catboost\\core.py:5184\u001b[0m, in \u001b[0;36mCatBoostClassifier.predict\u001b[1;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\u001b[0m\n\u001b[0;32m   5133\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mpredict\u001b[39m(\u001b[39mself\u001b[39m, data, prediction_type\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mClass\u001b[39m\u001b[39m'\u001b[39m, ntree_start\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, ntree_end\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m, thread_count\u001b[39m=\u001b[39m\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, task_type\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mCPU\u001b[39m\u001b[39m\"\u001b[39m):\n\u001b[0;32m   5134\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   5135\u001b[0m \u001b[39m    Predict with data.\u001b[39;00m\n\u001b[0;32m   5136\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   5182\u001b[0m \u001b[39m              with log probability for every class for each object.\u001b[39;00m\n\u001b[0;32m   5183\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 5184\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, \u001b[39m'\u001b[39;49m\u001b[39mpredict\u001b[39;49m\u001b[39m'\u001b[39;49m, task_type)\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\catboost\\core.py:2541\u001b[0m, in \u001b[0;36mCatBoost._predict\u001b[1;34m(self, data, prediction_type, ntree_start, ntree_end, thread_count, verbose, parent_method_name, task_type)\u001b[0m\n\u001b[0;32m   2539\u001b[0m \u001b[39mif\u001b[39;00m verbose \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m   2540\u001b[0m     verbose \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m-> 2541\u001b[0m data, data_is_single_object \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_process_predict_input_data(data, parent_method_name, thread_count)\n\u001b[0;32m   2542\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_prediction_type(prediction_type)\n\u001b[0;32m   2544\u001b[0m predictions \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_base_predict(data, prediction_type, ntree_start, ntree_end, thread_count, verbose, task_type)\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\catboost\\core.py:2521\u001b[0m, in \u001b[0;36mCatBoost._process_predict_input_data\u001b[1;34m(self, data, parent_method_name, thread_count, label)\u001b[0m\n\u001b[0;32m   2519\u001b[0m is_single_object \u001b[39m=\u001b[39m _is_data_single_object(data)\n\u001b[0;32m   2520\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(data, Pool):\n\u001b[1;32m-> 2521\u001b[0m     data \u001b[39m=\u001b[39m Pool(\n\u001b[0;32m   2522\u001b[0m         data\u001b[39m=\u001b[39;49m[data] \u001b[39mif\u001b[39;49;00m is_single_object \u001b[39melse\u001b[39;49;00m data,\n\u001b[0;32m   2523\u001b[0m         label\u001b[39m=\u001b[39;49mlabel,\n\u001b[0;32m   2524\u001b[0m         cat_features\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_cat_feature_indices() \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(data, FeaturesData) \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2525\u001b[0m         text_features\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_text_feature_indices() \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(data, FeaturesData) \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2526\u001b[0m         embedding_features\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_get_embedding_feature_indices() \u001b[39mif\u001b[39;49;00m \u001b[39mnot\u001b[39;49;00m \u001b[39misinstance\u001b[39;49m(data, FeaturesData) \u001b[39melse\u001b[39;49;00m \u001b[39mNone\u001b[39;49;00m,\n\u001b[0;32m   2527\u001b[0m         thread_count\u001b[39m=\u001b[39;49mthread_count\n\u001b[0;32m   2528\u001b[0m     )\n\u001b[0;32m   2529\u001b[0m \u001b[39mreturn\u001b[39;00m data, is_single_object\n",
      "File \u001b[1;32mc:\\Users\\chris\\OneDrive\\Skrivebord\\studie_repos\\applied_ML_faelles\\.venv\\lib\\site-packages\\catboost\\core.py:792\u001b[0m, in \u001b[0;36mPool.__init__\u001b[1;34m(self, data, label, cat_features, text_features, embedding_features, embedding_features_data, column_description, pairs, delimiter, has_header, ignore_csv_quoting, weight, group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count, log_cout, log_cerr)\u001b[0m\n\u001b[0;32m    786\u001b[0m         \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(feature_names, PATH_TYPES):\n\u001b[0;32m    787\u001b[0m             \u001b[39mraise\u001b[39;00m CatBoostError(\n\u001b[0;32m    788\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mfeature_names must be None or have non-string type when the pool is created from \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    789\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mpython objects.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    790\u001b[0m             )\n\u001b[1;32m--> 792\u001b[0m         \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_init(data, label, cat_features, text_features, embedding_features, embedding_features_data, pairs, weight,\n\u001b[0;32m    793\u001b[0m                    group_id, group_weight, subgroup_id, pairs_weight, baseline, timestamp, feature_names, feature_tags, thread_count)\n\u001b[0;32m    794\u001b[0m \u001b[39msuper\u001b[39m(Pool, \u001b[39mself\u001b[39m)\u001b[39m.\u001b[39m\u001b[39m__init__\u001b[39m()\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = CatBoostClassifier(\n",
    "                           random_state = 42, # For reproducibility\n",
    "                           verbose = 1000,\n",
    "                           early_stopping_rounds = 10 #, No default - Preventing overfitting\n",
    "                           )\n",
    "\n",
    "# Grid search\n",
    "# found the guide here: https://youtu.be/N4rqz8Z4XOM \n",
    "grid_search = GridSearchCV(\n",
    "                           estimator=model,\n",
    "                           param_grid = model_params_search,\n",
    "                           n_jobs=1,\n",
    "                           cv = 3,\n",
    "                           scoring = \"accuracy\",\n",
    "                           error_score=0\n",
    "                           )\n",
    "\n",
    "grid_result = grid_search.fit(X_val, y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(grid_result.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test model  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'nan_mode': Min\n",
      "'eval_metric': MultiClass\n",
      "'iterations': 720\n",
      "'sampling_frequency': PerTree\n",
      "'leaf_estimation_method': Newton\n",
      "'od_pval': 0\n",
      "'grow_policy': SymmetricTree\n",
      "'penalties_coefficient': 1\n",
      "'boosting_type': Plain\n",
      "'model_shrink_mode': Constant\n",
      "'feature_border_type': GreedyLogSum\n",
      "'bayesian_matrix_reg': 0.10000000149011612\n",
      "'eval_fraction': 0\n",
      "'force_unit_auto_pair_weights': False\n",
      "'l2_leaf_reg': 3\n",
      "'random_strength': 1\n",
      "'od_type': Iter\n",
      "'rsm': 1\n",
      "'boost_from_average': False\n",
      "'model_size_reg': 0.5\n",
      "'pool_metainfo_options': {'tags': {}}\n",
      "'use_best_model': True\n",
      "'od_wait': 6\n",
      "'class_names': [0, 1, 2]\n",
      "'random_seed': 42\n",
      "'depth': 2\n",
      "'posterior_sampling': False\n",
      "'border_count': 254\n",
      "'bagging_temperature': 1\n",
      "'classes_count': 0\n",
      "'auto_class_weights': None\n",
      "'sparse_features_conflict_fraction': 0\n",
      "'leaf_estimation_backtracking': AnyImprovement\n",
      "'best_model_min_trees': 1\n",
      "'model_shrink_rate': 0\n",
      "'min_data_in_leaf': 2\n",
      "'loss_function': MultiClass\n",
      "'learning_rate': 0.30000001192092896\n",
      "'score_function': Cosine\n",
      "'task_type': CPU\n",
      "'leaf_estimation_iterations': 1\n",
      "'bootstrap_type': Bayesian\n",
      "'max_leaves': 31\n"
     ]
    }
   ],
   "source": [
    "final_model = model.set_params(\n",
    "    grid_result.best_params_\n",
    ")\n",
    "# final_model.fit(np.concatenate(X_train, X_val), np.concatenate(y_train, y_val))\n",
    "final_model.fit(X_val, y_val)\n",
    "final_model_pred = final_model.predict(X_train)\n",
    "accuracy = accuracy_score(y_train, final_model_pred)\n",
    "print(accuracy * 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create final Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import\n",
    "# Data wrangling\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import pandas as pd\n",
    "# Model\n",
    "from catboost import CatBoostClassifier\n",
    "# performance measure\n",
    "from sklearn.metrics import accuracy_score\n",
    "# Grid search\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# ------ Load data ------\n",
    "# test data (no labels (y_test) since we that way can't see the results)\n",
    "X_test = np.load(\"../Common/data/X_test.npy\")\n",
    "# Validation and training\n",
    "X_train = np.load(\"../Common/data/X_train.npy\")\n",
    "y_train = np.load(\"../Common/data/y_train.npy\")\n",
    "\n",
    "# ------ Create and train model ------ \n",
    "final_model = CatBoostClassifier(\n",
    "                           verbose = 100, # Print training process every 100 iteration\n",
    "                           random_state = 42, # For reproducibility\n",
    "                           early_stopping_rounds = 10 #, No default - Preventing overfitting\n",
    "                           )\n",
    "                           \n",
    "best_params = grid_result.best_params_\n",
    "\n",
    "final_model = final_model.set_params(best_params)\n",
    "final_model.fit(X_train, y_train)\n",
    "\n",
    "# ------ Create and save predictions ------\n",
    "final_model_pred = final_model.predict(X_test)\n",
    "accuracy = accuracy_score(y_train, final_model_pred)\n",
    "\n",
    "\n",
    "\n",
    "y_test_hat = final_model.predict(X_test)\n",
    "y_test_hat_pd = pd.DataFrame({\n",
    "    'Id': list(range(len(y_test_hat))),\n",
    "    'Category': y_test_hat,\n",
    "})\n",
    "\n",
    "\n",
    "# After you make your predictions, you should submit them on the Kaggle webpage for our competition.\n",
    "# Below is a small check that your output has the right type and shape\n",
    "assert isinstance(y_test_hat_pd, pd.DataFrame)\n",
    "assert all(y_test_hat_pd.columns == ['Id', 'Category'])\n",
    "\n",
    "# If you pass the checks, the file is saved.\n",
    "y_test_hat_pd.to_csv('y_test_hat.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.7 ('.venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "fe82a64eee2b8d17df1c98106d8e23656cf3c584b17313c850b9d654a3f19002"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
