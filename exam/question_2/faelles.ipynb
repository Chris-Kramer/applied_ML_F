{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-28 15:03:13.705302: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "/Users/gustavchristensen/Documents/SDU/MSc. Data Science/3. Semester - DT/Anvendt Maskinlæring/applied_ML_faelles/assignment_2/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# ----- Tensorflow -----\n",
    "import tensorflow as tf\n",
    "import tensorflow_datasets as tfds\n",
    "from keras import regularizers as reg\n",
    "from keras import optimizers\n",
    "from keras.models import Model, load_model\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.utils.vis_utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras.layers import Input, Dropout, Dense, Conv2D, MaxPooling2D, Flatten, Concatenate, AveragePooling2D, Rescaling\n",
    "\n",
    "# ----- Utility functions -----\n",
    "from utils import load_data, plot_hist"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Dataset patch_camelyon: could not find data in /Users/gustavchristensen/Documents/SDU/MSc. Data Science/3. Semester - DT/Anvendt Maskinlæring/patch_camelyon. Please make sure to call dataset_builder.download_and_prepare(), or pass download=True to tfds.load() before trying to access the tf.data.Dataset object.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn [15], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m data_dir \u001b[39m=\u001b[39m \u001b[39m'\u001b[39m\u001b[39m/Users/gustavchristensen/Documents/SDU/MSc. Data Science/3. Semester - DT/Anvendt Maskinlæring/patch_camelyon\u001b[39m\u001b[39m'\u001b[39m\n\u001b[1;32m      2\u001b[0m BATCH_SIZE \u001b[39m=\u001b[39m \u001b[39m32\u001b[39m\n\u001b[0;32m----> 3\u001b[0m train, test, val \u001b[39m=\u001b[39m load_data(data_dir, perc\u001b[39m=\u001b[39;49m\u001b[39m5\u001b[39;49m, batch_size\u001b[39m=\u001b[39;49mBATCH_SIZE)\n",
      "File \u001b[0;32m~/Documents/SDU/MSc. Data Science/3. Semester - DT/Anvendt Maskinlæring/applied_ML_faelles/exam/question_2/utils.py:64\u001b[0m, in \u001b[0;36mload_data\u001b[0;34m(data_dir, perc, batch_size)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[39m# Load data set\u001b[39;00m\n\u001b[1;32m     63\u001b[0m tf\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mset_seed(\u001b[39m42\u001b[39m)\n\u001b[0;32m---> 64\u001b[0m train_df, test_df, val_df \u001b[39m=\u001b[39m tfds\u001b[39m.\u001b[39;49mload(\u001b[39m'\u001b[39;49m\u001b[39mpatch_camelyon\u001b[39;49m\u001b[39m'\u001b[39;49m,split\u001b[39m=\u001b[39;49m[\u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtrain[:\u001b[39;49m\u001b[39m{\u001b[39;49;00mperc\u001b[39m}\u001b[39;49;00m\u001b[39m%]\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mtest[:\u001b[39;49m\u001b[39m{\u001b[39;49;00mperc\u001b[39m}\u001b[39;49;00m\u001b[39m%]\u001b[39;49m\u001b[39m'\u001b[39;49m, \u001b[39mf\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mvalidation[:\u001b[39;49m\u001b[39m{\u001b[39;49;00mperc\u001b[39m}\u001b[39;49;00m\u001b[39m%]\u001b[39;49m\u001b[39m'\u001b[39;49m],\n\u001b[1;32m     65\u001b[0m                                       data_dir \u001b[39m=\u001b[39;49m data_dir,\n\u001b[1;32m     66\u001b[0m                                       download\u001b[39m=\u001b[39;49m\u001b[39mFalse\u001b[39;49;00m,\n\u001b[1;32m     67\u001b[0m                                       shuffle_files\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     69\u001b[0m train_dataset \u001b[39m=\u001b[39m train_df\u001b[39m.\u001b[39mmap(_convert_sample)\u001b[39m.\u001b[39mbatch(batch_size)\n\u001b[1;32m     70\u001b[0m test_dataset \u001b[39m=\u001b[39m test_df\u001b[39m.\u001b[39mmap(_convert_sample)\u001b[39m.\u001b[39mbatch(batch_size)\n",
      "File \u001b[0;32m~/Documents/SDU/MSc. Data Science/3. Semester - DT/Anvendt Maskinlæring/applied_ML_faelles/assignment_2/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/Documents/SDU/MSc. Data Science/3. Semester - DT/Anvendt Maskinlæring/applied_ML_faelles/assignment_2/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/load.py:629\u001b[0m, in \u001b[0;36mload\u001b[0;34m(name, split, data_dir, batch_size, shuffle_files, download, as_supervised, decoders, read_config, with_info, builder_kwargs, download_and_prepare_kwargs, as_dataset_kwargs, try_gcs)\u001b[0m\n\u001b[1;32m    626\u001b[0m as_dataset_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mshuffle_files\u001b[39m\u001b[39m'\u001b[39m, shuffle_files)\n\u001b[1;32m    627\u001b[0m as_dataset_kwargs\u001b[39m.\u001b[39msetdefault(\u001b[39m'\u001b[39m\u001b[39mread_config\u001b[39m\u001b[39m'\u001b[39m, read_config)\n\u001b[0;32m--> 629\u001b[0m ds \u001b[39m=\u001b[39m dbuilder\u001b[39m.\u001b[39;49mas_dataset(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mas_dataset_kwargs)\n\u001b[1;32m    630\u001b[0m \u001b[39mif\u001b[39;00m with_info:\n\u001b[1;32m    631\u001b[0m   \u001b[39mreturn\u001b[39;00m ds, dbuilder\u001b[39m.\u001b[39minfo\n",
      "File \u001b[0;32m~/Documents/SDU/MSc. Data Science/3. Semester - DT/Anvendt Maskinlæring/applied_ML_faelles/assignment_2/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/logging/__init__.py:169\u001b[0m, in \u001b[0;36m_FunctionDecorator.__call__\u001b[0;34m(self, function, instance, args, kwargs)\u001b[0m\n\u001b[1;32m    167\u001b[0m metadata \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_start_call()\n\u001b[1;32m    168\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 169\u001b[0m   \u001b[39mreturn\u001b[39;00m function(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    170\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m:\n\u001b[1;32m    171\u001b[0m   metadata\u001b[39m.\u001b[39mmark_error()\n",
      "File \u001b[0;32m~/Documents/SDU/MSc. Data Science/3. Semester - DT/Anvendt Maskinlæring/applied_ML_faelles/assignment_2/.venv/lib/python3.10/site-packages/tensorflow_datasets/core/dataset_builder.py:747\u001b[0m, in \u001b[0;36mDatasetBuilder.as_dataset\u001b[0;34m(self, split, batch_size, shuffle_files, decoders, read_config, as_supervised)\u001b[0m\n\u001b[1;32m    745\u001b[0m \u001b[39m# pylint: enable=line-too-long\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m tf\u001b[39m.\u001b[39mio\u001b[39m.\u001b[39mgfile\u001b[39m.\u001b[39mexists(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_dir):\n\u001b[0;32m--> 747\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mAssertionError\u001b[39;00m(\n\u001b[1;32m    748\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mDataset \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m: could not find data in \u001b[39m\u001b[39m%s\u001b[39;00m\u001b[39m. Please make sure to call \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    749\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mdataset_builder.download_and_prepare(), or pass download=True to \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    750\u001b[0m       \u001b[39m\"\u001b[39m\u001b[39mtfds.load() before trying to access the tf.data.Dataset object.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    751\u001b[0m       \u001b[39m%\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mname, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_data_dir_root)\n\u001b[1;32m    752\u001b[0m   )\n\u001b[1;32m    754\u001b[0m \u001b[39m# By default, return all splits\u001b[39;00m\n\u001b[1;32m    755\u001b[0m \u001b[39mif\u001b[39;00m split \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "\u001b[0;31mAssertionError\u001b[0m: Dataset patch_camelyon: could not find data in /Users/gustavchristensen/Documents/SDU/MSc. Data Science/3. Semester - DT/Anvendt Maskinlæring/patch_camelyon. Please make sure to call dataset_builder.download_and_prepare(), or pass download=True to tfds.load() before trying to access the tf.data.Dataset object."
     ]
    }
   ],
   "source": [
    "data_dir = '/Users/gustavchristensen/Documents/SDU/MSc. Data Science/3. Semester - DT/Anvendt Maskinlæring/patch_camelyon'\n",
    "BATCH_SIZE = 32\n",
    "train, test, val = load_data(data_dir, perc=5, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architectures\n",
    "Before we begin building complex architectures for the CNN model, we must establish a baseline classifier which we will attempt to beat. This common-sense baseline will tell us if we are moving in the right direction. In our case, we can choose a simple random classifier with a threshold of 0.5 since we have a binary classification problem where half of the samples belong to class A and the remaining 50% belong to class B. We could also specify a 'simple' machine learning algorithm as a baseline, but although we do not do this explicitly, we will aim to beat the trained models in question 1 as CNNs has some properties that make them superior to non-deep learning algorithms and feedforward neural networks (Chollet, 2021).\n",
    "\n",
    "At this point, the first thing we want to achieve is statistical power, i.e. to cross the threshold of our baseline classifier by developing a small model. To do so, we focus on parameters such as the loss function, batch_size, learning rate, etc. Due to the way we represent the labels, we will use the softmax activation and the loss function categorical cross entropy rather than sigmoid and binary cross entropy, however, this will not change the results of our models."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----- Relatively Simple Convnet from DLPR -----\n",
    "inputs = Input(shape=(96, 96, 3))\n",
    "x = Conv2D(filters=32, kernel_size=3, activation=\"relu\")(inputs)\n",
    "x = MaxPooling2D(pool_size=2)(x)\n",
    "x = Conv2D(filters=64, kernel_size=3, activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=2)(x)\n",
    "x = Conv2D(filters=128, kernel_size=3, activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=2)(x)\n",
    "x = Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = MaxPooling2D(pool_size=2)(x)\n",
    "x = Conv2D(filters=256, kernel_size=3, activation=\"relu\")(x)\n",
    "x = Flatten()(x)\n",
    "outputs = Dense(2, activation=\"softmax\")(x)\n",
    "model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "# ----- Model summary -----\n",
    "model.summary()\n",
    "\n",
    "# ----- Configure model -----\n",
    "model.compile(optimizer='adam',\n",
    "\t\t\t  loss='categorical_crossentropy',\n",
    "\t\t\t  metrics=['accuracy'])\n",
    "\n",
    "# ----- Train model -----\n",
    "history = model.fit(train,\n",
    "\t\t\t\t\tepochs = 2,\n",
    "\t\t\t\t\tvalidation_data= val)\n",
    "\n",
    "# ----- Plot performance -----\n",
    "plot_hist(history)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First model = Simple CNN from DLWP --> "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e72cf8e77b7d4326b9e75b3e7b443795cefd44c8f7193af81f99b8d1febeaf0b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
