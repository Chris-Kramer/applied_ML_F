{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andly\\.conda\\envs\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy import mean, std\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, make_scorer, classification_report, confusion_matrix\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_sample(image):\n",
        "    #image = tf.image.rgb_to_grayscale(image)\n",
        "    image = tf.image.resize(image,[32,32]).numpy()\n",
        "    image = image.reshape(1,-1)\n",
        "    return image\n",
        "    24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current data dir C:\\Users\\andly\n"
          ]
        }
      ],
      "source": [
        "data_dir = r'C:\\Users\\andly'\n",
        "print('Current data dir '+data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done Loading Data\n",
            "Done spliting data\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(88)\n",
        "ds1,ds2,ds3 = tfds.load('patch_camelyon',\n",
        "                    split=['train[:5%]','test[:5%]','validation[:5%]'],\n",
        "                    data_dir = data_dir,\n",
        "                    download=False,\n",
        "                    batch_size=-1, # All data...no batches needed \n",
        "                    as_supervised=True, # So that we easily can transform data to numpy format\n",
        "                    shuffle_files=True)\n",
        "print('Done Loading Data')\n",
        "\n",
        "\n",
        "\n",
        "print(\"Done spliting data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andly\\.conda\\envs\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:541: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of training data features (observations,features): (13107, 3072)\n",
            "Shape of training data labels (observations,): (13107,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andly\\.conda\\envs\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:541: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done spliting data\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andly\\.conda\\envs\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:541: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_dataset = tfds.as_numpy(ds1) # FULL DATA\n",
        "train_dataset_image = np.vstack(list(map(convert_sample,train_dataset[0]))) # <-- This is the X\n",
        "train_dataset_image_Scaled = StandardScaler(with_mean=0, with_std=1).fit_transform(train_dataset_image)\n",
        "train_dataset_label = train_dataset[1].reshape(-1,) # <-- This is y   \n",
        "print(f'Shape of training data features (observations,features): {train_dataset_image_Scaled.shape}')\n",
        "print(f'Shape of training data labels (observations,): {train_dataset_label.shape}')\n",
        "\n",
        "validation_dataset = tfds.as_numpy(ds3)\n",
        "validation_dataset_image = np.vstack(list(map(convert_sample,validation_dataset[0])))\n",
        "validation_dataset_image_Scaled = StandardScaler(with_mean=0, with_std=1).fit_transform(validation_dataset_image)\n",
        "validation_dataset_label = validation_dataset[1].reshape(-1,) \n",
        "\n",
        "test_dataset = tfds.as_numpy(ds2)\n",
        "test_dataset_image = np.vstack(list(map(convert_sample,test_dataset[0]))) # <-- X_test\n",
        "test_dataset_image_Scaled = StandardScaler(with_mean=0, with_std=1).fit_transform(test_dataset_image)\n",
        "test_dataset_label = test_dataset[1].reshape(-1,)\n",
        "print(\"Done spliting data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC # for Support Vector Classification model\n",
        "import plotly.express as px  # for data visualization\n",
        "import plotly.graph_objects as go # for data visualization\n",
        "\n",
        "\n",
        "# SVM Estimator\n",
        "\n",
        "# initialize the SVM model\n",
        "svm = SVC()\n",
        "\n",
        "# define the parameter grid for grid search\n",
        "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "# perform grid search using the SVM model and the parameter grid\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, verbose=4)\n",
        "#grid_search.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "\n",
        "# print the best parameters\n",
        "#print(\"Best parameters: \", grid_search.best_params_)\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [],
      "source": [
        "svm_tuned = SVC(C=0.1, kernel='rbf', random_state=42)\n",
        "\n",
        "svm_tuned.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "\n",
        "# make predictions on the training set\n",
        "y_train_pred = svm_tuned.predict(train_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the training set\n",
        "train_accuracy = accuracy_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "\n",
        "# Compute the precision on the training set\n",
        "train_precision = precision_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Precision:\", train_precision)\n",
        "\n",
        "# Compute the recall on the training set\n",
        "train_recall = recall_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Recall:\", train_recall)\n",
        "\n",
        "# Compute the F1-score on the training set\n",
        "train_f1 = f1_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training F1-score:\", train_f1)\n",
        "\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = svm_tuned.predict(validation_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the validation set\n",
        "val_accuracy = accuracy_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "\n",
        "# Compute the precision on the validation set\n",
        "val_precision = precision_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Precision:\", val_precision)\n",
        "\n",
        "# Compute the recall on the validation set\n",
        "val_recall = recall_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Recall:\", val_recall)\n",
        "\n",
        "# Compute the F1-score on the validation set\n",
        "val_f1 = f1_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation F1-score:\", val_f1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = svm_tuned.predict(test_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the test set\n",
        "test_accuracy = accuracy_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Compute the precision on the test set\n",
        "test_precision = precision_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Precision:\", test_precision)\n",
        "\n",
        "# Compute the recall on the test set\n",
        "test_recall = recall_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "\n",
        "# Compute the F1-score on the test set\n",
        "test_f1 = f1_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test F1-score:\", test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Validation Accuracy: 0.778998778998779\n",
            "Validation Precision: 0.7984496124031008\n",
            "Validation Recall: 0.75\n",
            "Validation F1-score: 0.7734668335419274\n",
            "Test Accuracy: 0.7661782661782662\n",
            "Test Precision: 0.7722513089005235\n",
            "Test Recall: 0.7384230287859824\n",
            "Test F1-score: 0.7549584133077414\n"
          ]
        }
      ],
      "source": [
        "# XGBOOST Estimator\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "#xgb_tuned = XGBClassifier(tree_method=\"gpu_hist\", gpu_id=0, seed=42)\n",
        "\n",
        "# Define the parameter grid for the grid search\n",
        "param_grid = {\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'learning_rate': [0.1, 0.2, 0.3],\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    \n",
        "}\n",
        "\n",
        "# Initialize the XGBClassifier\n",
        "\n",
        "\n",
        "# Initialize the GridSearchCV\n",
        "#grid_search = GridSearchCV(estimator=xgb_tuned, param_grid=param_grid, cv=3, verbose=4, return_train_score=True)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "#grid_search.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "\n",
        "\n",
        "xgb_hyper = XGBClassifier(tree_method=\"gpu_hist\", gpu_id=0, learning_rate=0.1, max_depth=5, n_estimators=300)\n",
        "\n",
        "xgb_hyper.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "# Print the best parameters\n",
        "#print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = xgb_hyper.predict(validation_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the validation set\n",
        "val_accuracy = accuracy_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "\n",
        "# Compute the precision on the validation set\n",
        "val_precision = precision_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Precision:\", val_precision)\n",
        "\n",
        "# Compute the recall on the validation set\n",
        "val_recall = recall_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Recall:\", val_recall)\n",
        "\n",
        "# Compute the F1-score on the validation set\n",
        "val_f1 = f1_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation F1-score:\", val_f1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = xgb_hyper.predict(test_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the test set\n",
        "test_accuracy = accuracy_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Compute the precision on the test set\n",
        "test_precision = precision_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Precision:\", test_precision)\n",
        "\n",
        "# Compute the recall on the test set\n",
        "test_recall = recall_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "\n",
        "# Compute the F1-score on the test set\n",
        "test_f1 = f1_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test F1-score:\", test_f1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9965667200732433\n",
            "Training Precision: 0.9964858670741024\n",
            "Training Recall: 0.996638141809291\n",
            "Training F1-score: 0.9965619986247994\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on the training set\n",
        "y_train_pred = xgb_hyper.predict(train_dataset_image_Scaled)\n",
        "# Compute the accuracy on the training set\n",
        "train_accuracy = accuracy_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "\n",
        "# Compute the precision on the training set\n",
        "train_precision = precision_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Precision:\", train_precision)\n",
        "\n",
        "# Compute the recall on the training set\n",
        "train_recall = recall_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Recall:\", train_recall)\n",
        "\n",
        "# Compute the F1-score on the training set\n",
        "train_f1 = f1_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training F1-score:\", train_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 161,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0.7515262515262515\n"
          ]
        }
      ],
      "source": [
        "# Ensemble Learning with non-deep learning\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Initialize the classifiers\n",
        "svm = SVC(kernel='rbf', C=1)\n",
        "rf = RandomForestClassifier(n_estimators=300)\n",
        "xgb = XGBClassifier(tree_method=\"gpu_hist\", gpu_id=0, max_depth=5, objective='binary:logistic', n_estimators=300, eta=0.05)\n",
        "gb = GradientBoostingClassifier(n_estimators=300)\n",
        "\n",
        "# Create the ensemble model\n",
        "ensemble = StackingClassifier(estimators=[('svm', svm), ('rf', rf), ('xgb', xgb), ('gb',gb)], final_estimator=LogisticRegression())\n",
        "\n",
        "# Fit the ensemble model to the training data\n",
        "ensemble.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 180,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9927519645990692\n",
            "Training Precision: 0.9871581809941079\n",
            "Training Recall: 0.9984718826405868\n",
            "Training F1-score: 0.9927828002734939\n",
            "Validation Accuracy: 0.753968253968254\n",
            "Validation Precision: 0.7952314165497896\n",
            "Validation Recall: 0.6881067961165048\n",
            "Validation F1-score: 0.7378009108653221\n",
            "Test Accuracy: 0.7515262515262515\n",
            "Test Precision: 0.7663043478260869\n",
            "Test Recall: 0.7058823529411765\n",
            "Test F1-score: 0.7348534201954398\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on the test data\n",
        "y_train_pred = ensemble.predict(train_dataset_image_Scaled)\n",
        "# Compute the accuracy on the training set\n",
        "train_accuracy = accuracy_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "\n",
        "# Compute the precision on the training set\n",
        "train_precision = precision_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Precision:\", train_precision)\n",
        "\n",
        "# Compute the recall on the training set\n",
        "train_recall = recall_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Recall:\", train_recall)\n",
        "\n",
        "# Compute the F1-score on the training set\n",
        "train_f1 = f1_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training F1-score:\", train_f1)\n",
        "\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = ensemble.predict(validation_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the validation set\n",
        "val_accuracy = accuracy_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "\n",
        "# Compute the precision on the validation set\n",
        "val_precision = precision_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Precision:\", val_precision)\n",
        "\n",
        "# Compute the recall on the validation set\n",
        "val_recall = recall_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Recall:\", val_recall)\n",
        "\n",
        "# Compute the F1-score on the validation set\n",
        "val_f1 = f1_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation F1-score:\", val_f1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = ensemble.predict(test_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the test set\n",
        "test_accuracy = accuracy_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Compute the precision on the test set\n",
        "test_precision = precision_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Precision:\", test_precision)\n",
        "\n",
        "# Compute the recall on the test set\n",
        "test_recall = recall_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "\n",
        "# Compute the F1-score on the test set\n",
        "test_f1 = f1_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test F1-score:\", test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 188,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Best parameters: {'max_depth': 20, 'min_samples_leaf': 2, 'min_samples_split': 10, 'n_estimators': 200}\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "BaseSearchCV.predict() takes 2 positional arguments but 3 were given",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[188], line 27\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mBest parameters:\u001b[39m\u001b[39m\"\u001b[39m, grid_search2\u001b[39m.\u001b[39mbest_params_)\n\u001b[0;32m     26\u001b[0m \u001b[39m# Make predictions on the training set\u001b[39;00m\n\u001b[1;32m---> 27\u001b[0m y_train_pred \u001b[39m=\u001b[39m grid_search2\u001b[39m.\u001b[39;49mpredict(train_dataset_image_Scaled, train_dataset_label)\n\u001b[0;32m     29\u001b[0m \u001b[39m# Compute the accuracy on the training set\u001b[39;00m\n\u001b[0;32m     30\u001b[0m train_accuracy \u001b[39m=\u001b[39m accuracy_score(train_dataset_label, y_train_pred)\n",
            "\u001b[1;31mTypeError\u001b[0m: BaseSearchCV.predict() takes 2 positional arguments but 3 were given"
          ]
        }
      ],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Define the parameter grid for the grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Initialize the GridSearchCV\n",
        "grid_search2 = GridSearchCV(rf, param_grid, cv=3)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "grid_search2.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "\n",
        "# Print the best parameters\n",
        "print(\"Best parameters:\", grid_search2.best_params_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 193,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.0\n",
            "Training Precision: 1.0\n",
            "Training Recall: 1.0\n",
            "Training F1-score: 1.0\n",
            "Validation Accuracy: 0.7435897435897436\n",
            "Validation Precision: 0.7829131652661064\n",
            "Validation Recall: 0.6783980582524272\n",
            "Test F1-score: 0.5009439899307742\n",
            "Test Accuracy: 0.7356532356532357\n",
            "Test Precision: 0.754874651810585\n",
            "Test Recall: 0.6783479349186483\n",
            "Test F1-score: 0.7145682267633487\n"
          ]
        }
      ],
      "source": [
        "# New rf with hyperparameters found\n",
        "rf_hyper = RandomForestClassifier(max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200)\n",
        "\n",
        "\n",
        "rf.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "# Make predictions on the training set\n",
        "y_train_pred = rf.predict(train_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the training set\n",
        "train_accuracy = accuracy_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "\n",
        "# Compute the precision on the training set\n",
        "train_precision = precision_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Precision:\", train_precision)\n",
        "\n",
        "# Compute the recall on the training set\n",
        "train_recall = recall_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Recall:\", train_recall)\n",
        "\n",
        "# Compute the F1-score on the training set\n",
        "train_f1 = f1_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training F1-score:\", train_f1)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = rf.predict(validation_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the validation set\n",
        "val_accuracy = accuracy_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "\n",
        "# Compute the precision on the validation set\n",
        "val_precision = precision_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Precision:\", val_precision)\n",
        "\n",
        "# Compute the recall on the validation set\n",
        "val_recall = recall_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Recall:\", val_recall)\n",
        "\n",
        "# Compute the F1-score on the test set\n",
        "val_f1 = f1_score(validation_dataset_label, y_test_pred)\n",
        "print(\"Test F1-score:\", val_f1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = rf.predict(test_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the test set\n",
        "test_accuracy = accuracy_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Compute the precision on the test set\n",
        "test_precision = precision_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Precision:\", test_precision)\n",
        "\n",
        "# Compute the recall on the test set\n",
        "test_recall = recall_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "\n",
        "# Compute the F1-score on the test set\n",
        "test_f1 = f1_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test F1-score:\", test_f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c6e1f3d6a0d0f05258877e269d3fdfe70c0e2eececb78df2a8644631659deb4f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
