{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andly\\.conda\\envs\\venv\\lib\\site-packages\\tqdm\\auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Num GPUs Available:  1\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from numpy import mean, std\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score, make_scorer, classification_report, confusion_matrix, precision_score, recall_score, f1_score\n",
        "from sklearn.metrics import mean_squared_error as MSE\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import cross_val_score\n",
        "from xgboost import XGBRegressor, XGBClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import svm\n",
        "from sklearn.svm import SVC # for Support Vector Classification model\n",
        "import plotly.express as px  # for data visualization\n",
        "import plotly.graph_objects as go # for data visualization\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "\n",
        "\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {},
      "outputs": [],
      "source": [
        "def convert_sample(image):\n",
        "    #image = tf.image.rgb_to_grayscale(image)\n",
        "    image = tf.image.resize(image,[64,64]).numpy()\n",
        "    image = image.reshape(1,-1)\n",
        "    return image\n",
        "    24"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current data dir C:\\Users\\andly\n"
          ]
        }
      ],
      "source": [
        "data_dir = r'C:\\Users\\andly'\n",
        "print('Current data dir '+data_dir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done Loading Data\n"
          ]
        }
      ],
      "source": [
        "tf.random.set_seed(88)\n",
        "ds1,ds2,ds3 = tfds.load('patch_camelyon',\n",
        "                    split=['train[:5%]','test[:5%]','validation[:5%]'],\n",
        "                    data_dir = data_dir,\n",
        "                    download=False,\n",
        "                    batch_size=-1, # All data...no batches needed \n",
        "                    as_supervised=True, # So that we easily can transform data to numpy format\n",
        "                    shuffle_files=True)\n",
        "print('Done Loading Data')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andly\\.conda\\envs\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:541: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of training data features (observations,features): (13107, 12288)\n",
            "Shape of training data labels (observations,): (13107,)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\andly\\.conda\\envs\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:541: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
            "  warnings.warn(\n",
            "c:\\Users\\andly\\.conda\\envs\\venv\\lib\\site-packages\\sklearn\\utils\\_param_validation.py:541: FutureWarning: Passing an int for a boolean parameter is deprecated in version 1.2 and won't be supported anymore in version 1.4.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Done spliting data\n"
          ]
        }
      ],
      "source": [
        "\n",
        "train_dataset = tfds.as_numpy(ds1) # FULL DATA\n",
        "train_dataset_image = np.vstack(list(map(convert_sample,train_dataset[0]))) # <-- This is the X\n",
        "train_dataset_image_Scaled = StandardScaler(with_mean=0, with_std=1).fit_transform(train_dataset_image)\n",
        "train_dataset_label = train_dataset[1].reshape(-1,) # <-- This is y   \n",
        "print(f'Shape of training data features (observations,features): {train_dataset_image_Scaled.shape}')\n",
        "print(f'Shape of training data labels (observations,): {train_dataset_label.shape}')\n",
        "\n",
        "validation_dataset = tfds.as_numpy(ds3)\n",
        "validation_dataset_image = np.vstack(list(map(convert_sample,validation_dataset[0])))\n",
        "validation_dataset_image_Scaled = StandardScaler(with_mean=0, with_std=1).fit_transform(validation_dataset_image)\n",
        "validation_dataset_label = validation_dataset[1].reshape(-1,) \n",
        "\n",
        "test_dataset = tfds.as_numpy(ds2)\n",
        "test_dataset_image = np.vstack(list(map(convert_sample,test_dataset[0]))) # <-- X_test\n",
        "test_dataset_image_Scaled = StandardScaler(with_mean=0, with_std=1).fit_transform(test_dataset_image)\n",
        "test_dataset_label = test_dataset[1].reshape(-1,)\n",
        "print(\"Done spliting data\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "# SVM Estimator\n",
        "\n",
        "# initialize the SVM model\n",
        "svm = SVC()\n",
        "\n",
        "# define the parameter grid for grid search\n",
        "param_grid = {'C': [0.1, 1, 10], 'kernel': ['linear', 'rbf']}\n",
        "# perform grid search using the SVM model and the parameter grid\n",
        "grid_search = GridSearchCV(svm, param_grid, cv=5, verbose=4)\n",
        "#grid_search.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "\n",
        "# print the best parameters\n",
        "#print(\"Best parameters: \", grid_search.best_params_)\n",
        "  \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.7878995956359197\n",
            "Training Precision: 0.7892714417460805\n",
            "Training Recall: 0.7846882640586798\n",
            "Training F1-score: 0.7869731800766284\n",
            "Validation Accuracy: 0.7545787545787546\n",
            "Validation Precision: 0.7783641160949868\n",
            "Validation Recall: 0.7160194174757282\n",
            "Validation F1-score: 0.7458912768647282\n",
            "Test Accuracy: 0.7527472527472527\n",
            "Test Precision: 0.7759103641456583\n",
            "Test Recall: 0.6933667083854819\n",
            "Test F1-score: 0.7323198942498348\n"
          ]
        }
      ],
      "source": [
        "svm_tuned = SVC(C=0.1, kernel='rbf')\n",
        "\n",
        "svm_tuned.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "\n",
        "# make predictions on the training set\n",
        "y_train_pred = svm_tuned.predict(train_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the training set\n",
        "train_accuracy = accuracy_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "\n",
        "# Compute the precision on the training set\n",
        "train_precision = precision_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Precision:\", train_precision)\n",
        "\n",
        "# Compute the recall on the training set\n",
        "train_recall = recall_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Recall:\", train_recall)\n",
        "\n",
        "# Compute the F1-score on the training set\n",
        "train_f1 = f1_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training F1-score:\", train_f1)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = svm_tuned.predict(validation_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the validation set\n",
        "val_accuracy = accuracy_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "\n",
        "# Compute the precision on the validation set\n",
        "val_precision = precision_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Precision:\", val_precision)\n",
        "\n",
        "# Compute the recall on the validation set\n",
        "val_recall = recall_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Recall:\", val_recall)\n",
        "\n",
        "# Compute the F1-score on the validation set\n",
        "val_f1 = f1_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation F1-score:\", val_f1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = svm_tuned.predict(test_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the test set\n",
        "test_accuracy = accuracy_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Compute the precision on the test set\n",
        "test_precision = precision_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Precision:\", test_precision)\n",
        "\n",
        "# Compute the recall on the test set\n",
        "test_recall = recall_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "\n",
        "# Compute the F1-score on the test set\n",
        "test_f1 = f1_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test F1-score:\", test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[23], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# Get the training and validation accuracy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m train_acc \u001b[39m=\u001b[39m svm_tuned\u001b[39m.\u001b[39;49mscore(train_dataset_image_Scaled, train_dataset_label)\n\u001b[0;32m      3\u001b[0m val_acc \u001b[39m=\u001b[39m svm_tuned\u001b[39m.\u001b[39mscore(validation_dataset_label, validation_dataset_label)\n",
            "File \u001b[1;32mc:\\Users\\andly\\.conda\\envs\\venv\\lib\\site-packages\\sklearn\\base.py:649\u001b[0m, in \u001b[0;36mClassifierMixin.score\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    624\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    625\u001b[0m \u001b[39mReturn the mean accuracy on the given test data and labels.\u001b[39;00m\n\u001b[0;32m    626\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    645\u001b[0m \u001b[39m    Mean accuracy of ``self.predict(X)`` wrt. `y`.\u001b[39;00m\n\u001b[0;32m    646\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m    647\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mmetrics\u001b[39;00m \u001b[39mimport\u001b[39;00m accuracy_score\n\u001b[1;32m--> 649\u001b[0m \u001b[39mreturn\u001b[39;00m accuracy_score(y, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mpredict(X), sample_weight\u001b[39m=\u001b[39msample_weight)\n",
            "File \u001b[1;32mc:\\Users\\andly\\.conda\\envs\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:820\u001b[0m, in \u001b[0;36mBaseSVC.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    818\u001b[0m     y \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39margmax(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecision_function(X), axis\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m    819\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 820\u001b[0m     y \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mpredict(X)\n\u001b[0;32m    821\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclasses_\u001b[39m.\u001b[39mtake(np\u001b[39m.\u001b[39masarray(y, dtype\u001b[39m=\u001b[39mnp\u001b[39m.\u001b[39mintp))\n",
            "File \u001b[1;32mc:\\Users\\andly\\.conda\\envs\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:435\u001b[0m, in \u001b[0;36mBaseLibSVM.predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    433\u001b[0m X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_for_predict(X)\n\u001b[0;32m    434\u001b[0m predict \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse_predict \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_sparse \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_dense_predict\n\u001b[1;32m--> 435\u001b[0m \u001b[39mreturn\u001b[39;00m predict(X)\n",
            "File \u001b[1;32mc:\\Users\\andly\\.conda\\envs\\venv\\lib\\site-packages\\sklearn\\svm\\_base.py:454\u001b[0m, in \u001b[0;36mBaseLibSVM._dense_predict\u001b[1;34m(self, X)\u001b[0m\n\u001b[0;32m    446\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    447\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mX.shape[1] = \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m should be equal to \u001b[39m\u001b[39m%d\u001b[39;00m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    448\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mthe number of samples at training time\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    449\u001b[0m             \u001b[39m%\u001b[39m (X\u001b[39m.\u001b[39mshape[\u001b[39m1\u001b[39m], \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mshape_fit_[\u001b[39m0\u001b[39m])\n\u001b[0;32m    450\u001b[0m         )\n\u001b[0;32m    452\u001b[0m svm_type \u001b[39m=\u001b[39m LIBSVM_IMPL\u001b[39m.\u001b[39mindex(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_impl)\n\u001b[1;32m--> 454\u001b[0m \u001b[39mreturn\u001b[39;00m libsvm\u001b[39m.\u001b[39;49mpredict(\n\u001b[0;32m    455\u001b[0m     X,\n\u001b[0;32m    456\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msupport_,\n\u001b[0;32m    457\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msupport_vectors_,\n\u001b[0;32m    458\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_n_support,\n\u001b[0;32m    459\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_dual_coef_,\n\u001b[0;32m    460\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_intercept_,\n\u001b[0;32m    461\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_probA,\n\u001b[0;32m    462\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_probB,\n\u001b[0;32m    463\u001b[0m     svm_type\u001b[39m=\u001b[39;49msvm_type,\n\u001b[0;32m    464\u001b[0m     kernel\u001b[39m=\u001b[39;49mkernel,\n\u001b[0;32m    465\u001b[0m     degree\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdegree,\n\u001b[0;32m    466\u001b[0m     coef0\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcoef0,\n\u001b[0;32m    467\u001b[0m     gamma\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_gamma,\n\u001b[0;32m    468\u001b[0m     cache_size\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcache_size,\n\u001b[0;32m    469\u001b[0m )\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Get the training and validation accuracy\n",
        "train_acc = svm_tuned.score(train_dataset_image_Scaled, train_dataset_label)\n",
        "val_acc = svm_tuned.score(validation_dataset_label, validation_dataset_label)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'val_acc' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[22], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m train_acc\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[1;32m----> 2\u001b[0m val_acc\u001b[39m.\u001b[39mreshape(\u001b[39m1\u001b[39m, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[39m# Plot the training and validation accuracy\u001b[39;00m\n\u001b[0;32m      5\u001b[0m plt\u001b[39m.\u001b[39mplot([\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m], [train_acc, val_acc], label\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mTraining Accuracy\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mValidation Accuracy\u001b[39m\u001b[39m'\u001b[39m])\n",
            "\u001b[1;31mNameError\u001b[0m: name 'val_acc' is not defined"
          ]
        }
      ],
      "source": [
        "train_acc.reshape(1, -1)\n",
        "val_acc.reshape(1, -1)\n",
        "\n",
        "# Plot the training and validation accuracy\n",
        "plt.plot([0, 1], [train_acc, val_acc], label=['Training Accuracy', 'Validation Accuracy'])\n",
        "plt.xlabel('Data')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'XGBClassifier' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[9], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[39m# XGBOOST Estimator\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m xgb_tuned \u001b[39m=\u001b[39m XGBClassifier(tree_method\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mgpu_hist\u001b[39m\u001b[39m\"\u001b[39m, gpu_id\u001b[39m=\u001b[39m\u001b[39m0\u001b[39m)\n\u001b[0;32m      5\u001b[0m \u001b[39m# Define the parameter grid for the grid search\u001b[39;00m\n\u001b[0;32m      6\u001b[0m param_grid \u001b[39m=\u001b[39m {\n\u001b[0;32m      7\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mmax_depth\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m5\u001b[39m],\n\u001b[0;32m      8\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mlearning_rate\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m0.1\u001b[39m, \u001b[39m0.01\u001b[39m, \u001b[39m0.001\u001b[39m],\n\u001b[0;32m      9\u001b[0m     \u001b[39m'\u001b[39m\u001b[39mn_estimators\u001b[39m\u001b[39m'\u001b[39m: [\u001b[39m50\u001b[39m, \u001b[39m100\u001b[39m, \u001b[39m500\u001b[39m],\n\u001b[0;32m     10\u001b[0m     \n\u001b[0;32m     11\u001b[0m }\n",
            "\u001b[1;31mNameError\u001b[0m: name 'XGBClassifier' is not defined"
          ]
        }
      ],
      "source": [
        "# XGBOOST Estimator\n",
        "\n",
        "xgb_tuned = XGBClassifier(tree_method=\"gpu_hist\", gpu_id=0)\n",
        "\n",
        "# Define the parameter grid for the grid search\n",
        "param_grid = {\n",
        "    'max_depth': [1, 2, 5],\n",
        "    'learning_rate': [0.1, 0.01, 0.001],\n",
        "    'n_estimators': [50, 100, 500],\n",
        "    \n",
        "}\n",
        "\n",
        "# Initialize the XGBClassifier\n",
        "\n",
        "\n",
        "#Initialize the GridSearchCV\n",
        "grid_search = GridSearchCV(estimator=xgb_tuned, param_grid=param_grid, cv=3, verbose=4, return_train_score=True)\n",
        "\n",
        "#Fit the GridSearchCV to the training data\n",
        "grid_search.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "\n",
        "\n",
        "# xgb_hyper = XGBClassifier(tree_method=\"gpu_hist\", gpu_id=0, learning_rate=0.1, max_depth=5, n_estimators=300)\n",
        "\n",
        "# xgb_hyper.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "# Print the best parameters\n",
        "#print(\"Best parameters:\", grid_search.best_params_)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = xgb_hyper.predict(validation_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the validation set\n",
        "val_accuracy = accuracy_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "\n",
        "# Compute the precision on the validation set\n",
        "val_precision = precision_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Precision:\", val_precision)\n",
        "\n",
        "# Compute the recall on the validation set\n",
        "val_recall = recall_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Recall:\", val_recall)\n",
        "\n",
        "# Compute the F1-score on the validation set\n",
        "val_f1 = f1_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation F1-score:\", val_f1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = xgb_hyper.predict(test_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the test set\n",
        "test_accuracy = accuracy_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Compute the precision on the test set\n",
        "test_precision = precision_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Precision:\", test_precision)\n",
        "\n",
        "# Compute the recall on the test set\n",
        "test_recall = recall_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "\n",
        "# Compute the F1-score on the test set\n",
        "test_f1 = f1_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test F1-score:\", test_f1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9965667200732433\n",
            "Training Precision: 0.9964858670741024\n",
            "Training Recall: 0.996638141809291\n",
            "Training F1-score: 0.9965619986247994\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on the training set\n",
        "y_train_pred = xgb_hyper.predict(train_dataset_image_Scaled)\n",
        "# Compute the accuracy on the training set\n",
        "train_accuracy = accuracy_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "\n",
        "# Compute the precision on the training set\n",
        "train_precision = precision_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Precision:\", train_precision)\n",
        "\n",
        "# Compute the recall on the training set\n",
        "train_recall = recall_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Recall:\", train_recall)\n",
        "\n",
        "# Compute the F1-score on the training set\n",
        "train_f1 = f1_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training F1-score:\", train_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/html": [
              "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>StackingClassifier(estimators=[(&#x27;svm&#x27;, SVC(C=1)),\n",
              "                               (&#x27;rf&#x27;, RandomForestClassifier(n_estimators=300)),\n",
              "                               (&#x27;xgb&#x27;,\n",
              "                                XGBClassifier(base_score=None, booster=None,\n",
              "                                              callbacks=None,\n",
              "                                              colsample_bylevel=None,\n",
              "                                              colsample_bynode=None,\n",
              "                                              colsample_bytree=None,\n",
              "                                              early_stopping_rounds=None,\n",
              "                                              enable_categorical=False,\n",
              "                                              eta=0.05, eval_metric=None,\n",
              "                                              feature_types=None, gamma=None,\n",
              "                                              gpu_id=0, grow_...\n",
              "                                              interaction_constraints=None,\n",
              "                                              learning_rate=None, max_bin=None,\n",
              "                                              max_cat_threshold=None,\n",
              "                                              max_cat_to_onehot=None,\n",
              "                                              max_delta_step=None, max_depth=5,\n",
              "                                              max_leaves=None,\n",
              "                                              min_child_weight=None,\n",
              "                                              missing=nan,\n",
              "                                              monotone_constraints=None,\n",
              "                                              n_estimators=300, n_jobs=None,\n",
              "                                              num_parallel_tree=None,\n",
              "                                              predictor=None, ...)),\n",
              "                               (&#x27;gb&#x27;,\n",
              "                                GradientBoostingClassifier(n_estimators=300))],\n",
              "                   final_estimator=LogisticRegression())</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">StackingClassifier</label><div class=\"sk-toggleable__content\"><pre>StackingClassifier(estimators=[(&#x27;svm&#x27;, SVC(C=1)),\n",
              "                               (&#x27;rf&#x27;, RandomForestClassifier(n_estimators=300)),\n",
              "                               (&#x27;xgb&#x27;,\n",
              "                                XGBClassifier(base_score=None, booster=None,\n",
              "                                              callbacks=None,\n",
              "                                              colsample_bylevel=None,\n",
              "                                              colsample_bynode=None,\n",
              "                                              colsample_bytree=None,\n",
              "                                              early_stopping_rounds=None,\n",
              "                                              enable_categorical=False,\n",
              "                                              eta=0.05, eval_metric=None,\n",
              "                                              feature_types=None, gamma=None,\n",
              "                                              gpu_id=0, grow_...\n",
              "                                              interaction_constraints=None,\n",
              "                                              learning_rate=None, max_bin=None,\n",
              "                                              max_cat_threshold=None,\n",
              "                                              max_cat_to_onehot=None,\n",
              "                                              max_delta_step=None, max_depth=5,\n",
              "                                              max_leaves=None,\n",
              "                                              min_child_weight=None,\n",
              "                                              missing=nan,\n",
              "                                              monotone_constraints=None,\n",
              "                                              n_estimators=300, n_jobs=None,\n",
              "                                              num_parallel_tree=None,\n",
              "                                              predictor=None, ...)),\n",
              "                               (&#x27;gb&#x27;,\n",
              "                                GradientBoostingClassifier(n_estimators=300))],\n",
              "                   final_estimator=LogisticRegression())</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>svm</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>rf</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">RandomForestClassifier</label><div class=\"sk-toggleable__content\"><pre>RandomForestClassifier(n_estimators=300)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>xgb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-4\" type=\"checkbox\" ><label for=\"sk-estimator-id-4\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">XGBClassifier</label><div class=\"sk-toggleable__content\"><pre>XGBClassifier(base_score=None, booster=None, callbacks=None,\n",
              "              colsample_bylevel=None, colsample_bynode=None,\n",
              "              colsample_bytree=None, early_stopping_rounds=None,\n",
              "              enable_categorical=False, eta=0.05, eval_metric=None,\n",
              "              feature_types=None, gamma=None, gpu_id=0, grow_policy=None,\n",
              "              importance_type=None, interaction_constraints=None,\n",
              "              learning_rate=None, max_bin=None, max_cat_threshold=None,\n",
              "              max_cat_to_onehot=None, max_delta_step=None, max_depth=5,\n",
              "              max_leaves=None, min_child_weight=None, missing=nan,\n",
              "              monotone_constraints=None, n_estimators=300, n_jobs=None,\n",
              "              num_parallel_tree=None, predictor=None, ...)</pre></div></div></div></div></div></div><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>gb</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-5\" type=\"checkbox\" ><label for=\"sk-estimator-id-5\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingClassifier</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingClassifier(n_estimators=300)</pre></div></div></div></div></div></div></div></div><div class=\"sk-item\"><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><label>final_estimator</label></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" ><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression()</pre></div></div></div></div></div></div></div></div></div></div></div></div>"
            ],
            "text/plain": [
              "StackingClassifier(estimators=[('svm', SVC(C=1)),\n",
              "                               ('rf', RandomForestClassifier(n_estimators=300)),\n",
              "                               ('xgb',\n",
              "                                XGBClassifier(base_score=None, booster=None,\n",
              "                                              callbacks=None,\n",
              "                                              colsample_bylevel=None,\n",
              "                                              colsample_bynode=None,\n",
              "                                              colsample_bytree=None,\n",
              "                                              early_stopping_rounds=None,\n",
              "                                              enable_categorical=False,\n",
              "                                              eta=0.05, eval_metric=None,\n",
              "                                              feature_types=None, gamma=None,\n",
              "                                              gpu_id=0, grow_...\n",
              "                                              interaction_constraints=None,\n",
              "                                              learning_rate=None, max_bin=None,\n",
              "                                              max_cat_threshold=None,\n",
              "                                              max_cat_to_onehot=None,\n",
              "                                              max_delta_step=None, max_depth=5,\n",
              "                                              max_leaves=None,\n",
              "                                              min_child_weight=None,\n",
              "                                              missing=nan,\n",
              "                                              monotone_constraints=None,\n",
              "                                              n_estimators=300, n_jobs=None,\n",
              "                                              num_parallel_tree=None,\n",
              "                                              predictor=None, ...)),\n",
              "                               ('gb',\n",
              "                                GradientBoostingClassifier(n_estimators=300))],\n",
              "                   final_estimator=LogisticRegression())"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Ensemble Learning with non-deep learning\n",
        "\n",
        "import numpy as np\n",
        "from sklearn.ensemble import StackingClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.ensemble import GradientBoostingClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# # Initialize the classifiers\n",
        "# svm = SVC(kernel='rbf', C=1)\n",
        "# rf = RandomForestClassifier(n_estimators=300)\n",
        "# xgb = XGBClassifier(tree_method=\"gpu_hist\", gpu_id=0, max_depth=5, objective='binary:logistic', n_estimators=300, eta=0.05)\n",
        "# gb = GradientBoostingClassifier(n_estimators=300)\n",
        "\n",
        "# Initialize the classifiers\n",
        "rf = RandomForestClassifier(n_estimators=300)\n",
        "xgb = XGBClassifier(tree_method=\"gpu_hist\", gpu_id=0, max_depth=5, n_estimators=360, eta=0.05)\n",
        "gb = GradientBoostingClassifier(n_estimators=300)\n",
        "\n",
        "\n",
        "# Create the ensemble model\n",
        "ensemble = StackingClassifier(estimators=[('rf', rf), ('xgb', xgb), ('gb',gb)], final_estimator=LogisticRegression())\n",
        "\n",
        "# Fit the ensemble model to the training data\n",
        "ensemble.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 0.9770351720454719\n",
            "Training Precision: 0.9721675994554531\n",
            "Training Recall: 0.9821210268948656\n",
            "Training F1-score: 0.9771189661725579\n",
            "Validation Accuracy: 0.7741147741147741\n",
            "Validation Precision: 0.8026666666666666\n",
            "Validation Recall: 0.7305825242718447\n",
            "Validation F1-score: 0.7649301143583227\n",
            "Test Accuracy: 0.7667887667887668\n",
            "Test Precision: 0.7697283311772316\n",
            "Test Recall: 0.7446808510638298\n",
            "Test F1-score: 0.7569974554707378\n"
          ]
        }
      ],
      "source": [
        "# Make predictions on the test data\n",
        "y_train_pred = ensemble.predict(train_dataset_image_Scaled)\n",
        "# Compute the accuracy on the training set\n",
        "train_accuracy = accuracy_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "\n",
        "# Compute the precision on the training set\n",
        "train_precision = precision_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Precision:\", train_precision)\n",
        "\n",
        "# Compute the recall on the training set\n",
        "train_recall = recall_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Recall:\", train_recall)\n",
        "\n",
        "# Compute the F1-score on the training set\n",
        "train_f1 = f1_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training F1-score:\", train_f1)\n",
        "\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = ensemble.predict(validation_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the validation set\n",
        "val_accuracy = accuracy_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "\n",
        "# Compute the precision on the validation set\n",
        "val_precision = precision_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Precision:\", val_precision)\n",
        "\n",
        "# Compute the recall on the validation set\n",
        "val_recall = recall_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Recall:\", val_recall)\n",
        "\n",
        "# Compute the F1-score on the validation set\n",
        "val_f1 = f1_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation F1-score:\", val_f1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = ensemble.predict(test_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the test set\n",
        "test_accuracy = accuracy_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Compute the precision on the test set\n",
        "test_precision = precision_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Precision:\", test_precision)\n",
        "\n",
        "# Compute the recall on the test set\n",
        "test_recall = recall_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "\n",
        "# Compute the F1-score on the test set\n",
        "test_f1 = f1_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test F1-score:\", test_f1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "\n",
        "# Define the parameter grid for the grid search\n",
        "param_grid = {\n",
        "    'n_estimators': [50, 100, 200],\n",
        "    'max_depth': [3, 10, 20],\n",
        "    'min_samples_split': [2, 5, 10],\n",
        "    'min_samples_leaf': [1, 2, 4],\n",
        "}\n",
        "\n",
        "# Initialize the RandomForestClassifier\n",
        "rf = RandomForestClassifier()\n",
        "\n",
        "# Initialize the GridSearchCV\n",
        "grid_search2 = GridSearchCV(rf, param_grid, cv=3)\n",
        "\n",
        "# Fit the GridSearchCV to the training data\n",
        "#grid_search2.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "\n",
        "# Print the best parameters\n",
        "#print(\"Best parameters:\", grid_search2.best_params_)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training Accuracy: 1.0\n",
            "Training Precision: 1.0\n",
            "Training Recall: 1.0\n",
            "Training F1-score: 1.0\n",
            "Validation Accuracy: 0.7509157509157509\n",
            "Validation Precision: 0.7849315068493151\n",
            "Validation Recall: 0.6953883495145631\n",
            "Val F1-score: 0.48966408268733846\n",
            "Test Accuracy: 0.7527472527472527\n",
            "Test Precision: 0.7743732590529248\n",
            "Test Recall: 0.6958698372966208\n",
            "Test F1-score: 0.7330257086354647\n"
          ]
        }
      ],
      "source": [
        "# New rf with hyperparameters found\n",
        "rf_hyper = RandomForestClassifier(max_depth=20, min_samples_leaf=2, min_samples_split=10, n_estimators=200)\n",
        "\n",
        "\n",
        "rf.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "# Make predictions on the training set\n",
        "y_train_pred = rf.predict(train_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the training set\n",
        "train_accuracy = accuracy_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Accuracy:\", train_accuracy)\n",
        "\n",
        "# Compute the precision on the training set\n",
        "train_precision = precision_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Precision:\", train_precision)\n",
        "\n",
        "# Compute the recall on the training set\n",
        "train_recall = recall_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training Recall:\", train_recall)\n",
        "\n",
        "# Compute the F1-score on the training set\n",
        "train_f1 = f1_score(train_dataset_label, y_train_pred)\n",
        "print(\"Training F1-score:\", train_f1)\n",
        "\n",
        "# Make predictions on the validation set\n",
        "y_val_pred = rf.predict(validation_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the validation set\n",
        "val_accuracy = accuracy_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Accuracy:\", val_accuracy)\n",
        "\n",
        "# Compute the precision on the validation set\n",
        "val_precision = precision_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Precision:\", val_precision)\n",
        "\n",
        "# Compute the recall on the validation set\n",
        "val_recall = recall_score(validation_dataset_label, y_val_pred)\n",
        "print(\"Validation Recall:\", val_recall)\n",
        "\n",
        "# Compute the F1-score on the test set\n",
        "val_f1 = f1_score(validation_dataset_label, y_test_pred)\n",
        "print(\"Val F1-score:\", val_f1)\n",
        "\n",
        "# Make predictions on the test set\n",
        "y_test_pred = rf.predict(test_dataset_image_Scaled)\n",
        "\n",
        "# Compute the accuracy on the test set\n",
        "test_accuracy = accuracy_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Accuracy:\", test_accuracy)\n",
        "\n",
        "# Compute the precision on the test set\n",
        "test_precision = precision_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Precision:\", test_precision)\n",
        "\n",
        "# Compute the recall on the test set\n",
        "test_recall = recall_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test Recall:\", test_recall)\n",
        "\n",
        "# Compute the F1-score on the test set\n",
        "test_f1 = f1_score(test_dataset_label, y_test_pred)\n",
        "print(\"Test F1-score:\", test_f1)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "c6e1f3d6a0d0f05258877e269d3fdfe70c0e2eececb78df2a8644631659deb4f"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
