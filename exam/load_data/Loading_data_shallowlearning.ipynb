{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WhvLML4T15tV"
      },
      "source": [
        "#### Loading and preparing the PCam data for training shallow learning models using tensorflow dataset (tfds)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JwxQHGy_15tb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "2023-01-24 15:27:35.434005: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
            "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "/Users/gustavchristensen/Documents/SDU/MSc. Data Science/3. Semester - DT/Anvendt Maskinlæring/applied_ML_faelles/assignment_2/.venv/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "from sklearn import svm\n",
        "from sklearn.metrics import accuracy_score\n",
        "import numpy as np\n",
        "\n",
        "import tensorflow as tf\n",
        "import tensorflow_datasets as tfds\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lOKqkr5U15tg"
      },
      "source": [
        "Defining a function that grayscale, resize and flattens the image. This function might also become handy (for deep learning) if the original images are too large for your hardware configuration."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "u2HYPeO615th"
      },
      "outputs": [],
      "source": [
        "def convert_sample(image):\n",
        "    image = tf.image.rgb_to_grayscale(image)\n",
        "    image = tf.image.resize(image,[32,32]).numpy()\n",
        "    image = image.reshape(1,-1)\n",
        "    return image"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VBn2P6Yo15tj"
      },
      "source": [
        "Next we use the tensorflow dataset API - tfds - to load data from your mounted google drive. Note this API requite that you should have copied the entire **patch_camelyon** folder from https://syddanskuni-my.sharepoint.com/:f:/g/personal/cmd_sam_sdu_dk/EiWD2LmuxCJBp-_tfGK7aL8Bair7l5z8FU5sp5pLjlhKwg?e=FLzWno to the /content/drive/MyDrive folder on your google drive:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "ajHSLb1e15tl"
      },
      "outputs": [],
      "source": [
        "ds1,ds2,ds3 = tfds.load('patch_camelyon',\n",
        "                        split=['train[:10%]','test[:10%]','validation[:10%]'],\n",
        "                        data_dir = '/Users/gustavchristensen/Documents/SDU/MSc. Data Science/3. Semester - DT/Anvendt Maskinlæring',\n",
        "                        download=False,\n",
        "                        batch_size=-1, # All data...no batches needed \n",
        "                        as_supervised=True, # So that we easily can transform data to numpy format\n",
        "                        shuffle_files=True)\n",
        "                        "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9kuVMFWJ15tn"
      },
      "source": [
        "Next we can easily convert both the images and the labels to numpy format "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ROyKRTvB15to",
        "outputId": "da37ff05-fc86-475a-f3d4-626445e7b0c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Shape of training data features (observations,features): (20972, 1024)\n",
            "Shape of training data labels (observations,): (20972,)\n"
          ]
        }
      ],
      "source": [
        "train_dataset       = tfds.as_numpy(ds1)\n",
        "train_dataset_image = np.vstack(list(map(convert_sample,train_dataset[0])))\n",
        "train_dataset_image_Scaled = StandardScaler(with_mean=0, with_std=1).fit_transform(train_dataset_image)\n",
        "train_dataset_label = train_dataset[1].reshape(-1,)    \n",
        "print(f'Shape of training data features (observations,features): {train_dataset_image_Scaled.shape}')\n",
        "print(f'Shape of training data labels (observations,): {train_dataset_label.shape}')\n",
        "\n",
        "validation_dataset  = tfds.as_numpy(ds3)\n",
        "validation_dataset_image = np.vstack(list(map(convert_sample,validation_dataset[0])))\n",
        "validation_dataset_image_Scaled = StandardScaler(with_mean=0, with_std=1).fit_transform(validation_dataset_image)\n",
        "validation_dataset_label = validation_dataset[1].reshape(-1,) \n",
        "   \n",
        "test_dataset        = tfds.as_numpy(ds2)\n",
        "test_dataset_image = np.vstack(list(map(convert_sample,test_dataset[0])))\n",
        "test_dataset_image_Scaled = StandardScaler(with_mean=0, with_std=1).fit_transform(test_dataset_image)\n",
        "test_dataset_label = test_dataset[1].reshape(-1,)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WZ4KbAKP15tr"
      },
      "source": [
        "The data is then ready to be applied for training, validation, testing in a shallow learning model such as the SVM classifier...below just a very very simple illustration on how to construct and train a support vector machine based on the data we have prepared"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iw5goz8g15tt",
        "outputId": "059566ce-5769-429d-f6ce-67c36d3b9c85"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SVM achieved 51.9% accuracy.\n"
          ]
        }
      ],
      "source": [
        "clf = svm.SVC(kernel='rbf')\n",
        "clf.fit(train_dataset_image_Scaled, train_dataset_label)\n",
        "y_test_hat = clf.predict(test_dataset_image)\n",
        "\n",
        "# Obtain accuracy by using the `accuracy_score` function\n",
        "accuracy_linear = accuracy_score(y_test_hat, test_dataset_label )\n",
        "# Print results\n",
        "print(f'SVM achieved {round(accuracy_linear * 100, 1)}% accuracy.')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.2"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "e72cf8e77b7d4326b9e75b3e7b443795cefd44c8f7193af81f99b8d1febeaf0b"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
